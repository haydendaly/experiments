{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json\n",
    "\n",
    "def searchRepos(search):\n",
    "    req = \"https://api.github.com/search/repositories?q=\" + search\n",
    "    res = requests.get(req).json()\n",
    "    return res['items']\n",
    "\n",
    "def getModules(string):\n",
    "    req = string.replace(\"github\", \"raw.githubusercontent\") + '/master/package.json'\n",
    "    res = requests.get(req).json()\n",
    "    modules = list(res['devDependencies'].keys())\n",
    "    return modules\n",
    "    \n",
    "    \n",
    "def parseInfo(repoList):\n",
    "    descModulePairings = []\n",
    "    for i in range(len(repoList)):\n",
    "        try:\n",
    "            descModulePairings.append([repoList[i]['description'], getModules(repoList[i]['html_url'])])\n",
    "        except:\n",
    "            pass\n",
    "    return descModulePairings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "repoList = searchRepos('language:js')\n",
    "descModulePairings = parseInfo(repoList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/hayden/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "stemmer = SnowballStemmer('english')\n",
    "from langdetect import detect\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanData(pairings):\n",
    "    newPairings = []\n",
    "    for i in range(len(pairings)):\n",
    "        if detect(pairings[i][0]) != 'en':\n",
    "            pass\n",
    "        pairings[i][0] = tokenizer.tokenize(pairings[i][0].lower())\n",
    "        pairings[i][0] = [stemmer.stem(word) for word in pairings[i][0] if (word not in stop_words)]\n",
    "        newPairings.append(pairings[i])\n",
    "    return(newPairings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempPairs = copy.deepcopy(descModulePairings)\n",
    "pairings = cleanData(tempPairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restructure(pairings):\n",
    "    data = {}\n",
    "    for i in range(len(pairings)):\n",
    "        for module in pairings[i][1]:\n",
    "            if module in data.keys():\n",
    "                data[module] += pairings[i][0]\n",
    "            else:\n",
    "                data[module] = pairings[i][0]\n",
    "    for key in data:\n",
    "        data[key] = list(dict.fromkeys(data[key]))\n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "moduleDict = restructure(pairings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def removeCommonWords(moduleDict):\n",
    "    allWords = []\n",
    "    for key in moduleDict:\n",
    "        for word in moduleDict[key]:\n",
    "            allWords.append(word)\n",
    "    count = Counter(allWords)\n",
    "    mostCommon = count.most_common(int(len(count)*.25))\n",
    "    mostCommonList = [mostCommon[i][0] for i in range(len(mostCommon))]\n",
    "    for key in moduleDict:\n",
    "        for word in moduleDict[key]:\n",
    "            if word in mostCommonList:\n",
    "                moduleDict[key].remove(word)\n",
    "        moduleDict[key] = \" \".join(moduleDict[key])\n",
    "    return moduleDict, mostCommonList\n",
    "\n",
    "newModuleDict, mostCommonList = removeCommonWords(moduleDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bagOfWords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>@freecodecamp/eslint-config</th>\n",
       "      <td>freecodecamp org open sourc codebas curriculum...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>babel-eslint</th>\n",
       "      <td>freecodecamp org open sourc codebas curriculum...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cross-env</th>\n",
       "      <td>freecodecamp org open sourc codebas curriculum...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>debug</th>\n",
       "      <td>freecodecamp org open sourc codebas curriculum...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>docsify-cli</th>\n",
       "      <td>freecodecamp org open sourc codebas curriculum...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                    bagOfWords\n",
       "@freecodecamp/eslint-config  freecodecamp org open sourc codebas curriculum...\n",
       "babel-eslint                 freecodecamp org open sourc codebas curriculum...\n",
       "cross-env                    freecodecamp org open sourc codebas curriculum...\n",
       "debug                        freecodecamp org open sourc codebas curriculum...\n",
       "docsify-cli                  freecodecamp org open sourc codebas curriculum..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame.from_dict(moduleDict, orient='index')\n",
    "df.columns = ['bagOfWords']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         1.         1.         ... 0.23791548 0.23791548 0.23791548]\n",
      " [1.         1.         1.         ... 0.23791548 0.23791548 0.23791548]\n",
      " [1.         1.         1.         ... 0.23791548 0.23791548 0.23791548]\n",
      " ...\n",
      " [0.23791548 0.23791548 0.23791548 ... 1.         1.         1.        ]\n",
      " [0.23791548 0.23791548 0.23791548 ... 1.         1.         1.        ]\n",
      " [0.23791548 0.23791548 0.23791548 ... 1.         1.         1.        ]]\n"
     ]
    }
   ],
   "source": [
    "count = CountVectorizer()\n",
    "count_matrix = count.fit_transform(df['bagOfWords'])\n",
    "cosine_sim = cosine_similarity(count_matrix, count_matrix)\n",
    "print(cosine_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanUp(string):\n",
    "    words = tokenizer.tokenize(string.lower())\n",
    "    stemmedWords = [stemmer.stem(word) for word in words if (word not in stop_words)]\n",
    "    for word in stemmedWords:\n",
    "        if word in mostCommonList:\n",
    "            stemmedWords.remove(word)\n",
    "    return \" \".join(stemmedWords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommendSimilarModules(title, amount, cosine_sim = cosine_sim):\n",
    "    indices = pd.Series(df.index)\n",
    "    recommended_modules = []\n",
    "    i = indices[indices == title].index[0]\n",
    "    score_series = pd.Series(cosine_sim[i]).sort_values(ascending = False)\n",
    "    top_indices = list(score_series.iloc[1:amount].index)\n",
    "    for j in top_indices:\n",
    "        recommended_modules.append(list(df.index)[j])\n",
    "    return recommended_modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommendModules(description, amount, cosine_sim = cosine_sim):\n",
    "    cleanData = cleanUp(description)\n",
    "    count = CountVectorizer()\n",
    "    dfCopy = df.copy()\n",
    "    dfCopy.loc[\"TEMP_SUBJECT\"] = cleanData\n",
    "    count_matrix = count.fit_transform(dfCopy['bagOfWords'])\n",
    "    cosine_sim = cosine_similarity(count_matrix, count_matrix)\n",
    "    indices = pd.Series(dfCopy.index)\n",
    "    recommended_modules = []\n",
    "    score_series = pd.Series(cosine_sim[len(cosine_sim) - 1]).sort_values(ascending = False)\n",
    "    top_indices = list(score_series.iloc[1:amount].index)\n",
    "    for j in top_indices:\n",
    "        recommended_modules.append(list(df.index)[j])\n",
    "    return recommended_modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['normalize.css',\n",
       " 'apache-server-configs',\n",
       " 'archiver',\n",
       " 'del',\n",
       " 'eslint-config-recommended',\n",
       " 'gulp-autoprefixer',\n",
       " 'gulp-header',\n",
       " 'gulp-load-plugins',\n",
       " 'main.css',\n",
       " 'modernizr',\n",
       " 'ssri',\n",
       " 'grunt-contrib-uglify',\n",
       " 'grunt-compare-size',\n",
       " 'raw-body',\n",
       " 'grunt-git-authors',\n",
       " 'grunt-jsonlint',\n",
       " 'grunt-newer',\n",
       " 'gzip-js',\n",
       " 'insight',\n",
       " 'karma-jsdom-launcher',\n",
       " 'karma-qunit',\n",
       " 'native-promise-only',\n",
       " 'testswarm',\n",
       " 'strip-json-comments']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# recommendSimilarModules('grunt', 250)\n",
    "recommendModules('javascript template for message queues or frontend react app', 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "ds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
